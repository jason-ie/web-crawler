#!/usr/bin/env python3
import argparse
import socket
import ssl
from html.parser import HTMLParser
from urllib.parse import urljoin, urlparse

DEFAULT_SERVER = "www.3700.network"
DEFAULT_PORT = 443

class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password
        self.csrf_token = None
        self.ssl_ctxt = ssl.create_default_context()
        self.cookies = {}  
        self.url_visited = set()
        self.url_visited.add("https://www.3700.network/accounts/logout/")
        self.secret_flags = set()  
        self.need_to_visit = ["/fakebook/"]

    def run(self):
        """
        Run the crawler
        """
        self.get_csrf_token() # Get the CSRF token
        if self.csrf_token: # If the CSRF token was found
            self.login() # Log in
            self.crawl() # Start crawling
        else:
            print("CSRF token not found.")

    def get_csrf_token(self):
        response = self.send_request("GET", "/accounts/login/?next=/fakebook/") # Get the login page
        self.handle_cookies(response) # Update the cookies

        start_token = response.find('csrfmiddlewaretoken') + len('csrfmiddlewaretoken') # Find the CSRF token
        start_token = response.find('value="', start_token) + len('value="') # Find the start of the token
        end_token = response.find('"', start_token) # Find the end of the token
        self.csrf_token = response[start_token:end_token] # Extract the token

    def login(self):
        """
        Log in to the server
        """
        self.cookies['csrftoken'] = self.csrf_token # Add the CSRF token to the cookies 
        post_data = f"username={self.username}&password={self.password}&csrfmiddlewaretoken={self.csrf_token}&next=/fakebook/" # Create the POST data
        response = self.send_request("POST", "/accounts/login/?next=/fakebook/", post_data=post_data) # Send the POST request
        self.handle_cookies(response) # Update the cookies

    def send_request(self, method, path, post_data=None):
        """
        Send a request to the server

        Args:
            method (_type_): _description_
            path (_type_): _description_
            post_data (_type_, optional): _description_. Defaults to None.

        Returns:
            _type_: _description_
        """
        cookies = "; ".join([f"{key}={value}" for key, value in self.cookies.items()]) # Create the cookie header
        headers = {
            "Host": self.server, # Add the host header
            "Connection": "close", # Add the connection header
            "Content-Type": "application/x-www-form-urlencoded", # Add the content type header
            "Cookie": cookies  # Add the cookie header
        }

        if post_data: # If there is POST data
            headers["Content-Length"] = str(len(post_data)) # Add the content length header
            request_body = post_data # Set the request body
        else:
            request_body = "" # Set the request body to an empty string

        request_lines = [f"{method} {path} HTTP/1.1"] + [f"{key}: {value}" for key, value in headers.items()] + ["", request_body] # Create the request
        request = "\r\n".join(request_lines) + "\r\n" # Join the request lines

        with socket.create_connection((self.server, self.port)) as sock: # Create a socket connection
            with self.ssl_ctxt.wrap_socket(sock, server_hostname=self.server) as ssock: # Wrap the socket in an SSL context
                ssock.sendall(request.encode('utf-8')) # Send the request
                response = self.receive_full_response(ssock) # Receive the response
                
                if "HTTP/1.1 302 Found" in response.split("\r\n")[0]: # If the response is a redirect
                    for line in response.split("\r\n"): # Iterate over the response lines
                        if line.startswith("Location:"): # If the line is the location header
                            location = line.split(" ")[1] # Get the location
                            return self.send_request("GET", urlparse(location).path) # Send a GET request to the location
                return response 

    def handle_cookies(self, response):
        """
        Update the cookies

        Args:
            response (_type_): _description_
        """
        lines = response.split("\r\n") # Split the response into lines
        for line in lines: # Iterate over the lines
            if line.startswith("set-cookie:"): # If the line is a set-cookie header
                cookie = line.split("set-cookie: ")[1].split(";")[0] # Get the cookie
                key, value = cookie.split("=") # Split the cookie into key and value
                self.cookies[key] = value # Add the cookie to the session cookies

    def receive_full_response(self, secure_socket):
        """
        Receive the full response
        
        Args:
            secure_socket (_type_): _description_

        Returns:
            _type_: _description_
        """
        
        buffer = bytearray() # Create a buffer
        while True:
            part = secure_socket.recv(4096) # Receive a part of the response
            if not part:
                break
            buffer.extend(part) # Add the part to the buffer
        response = buffer.decode('utf-8') # Decode the buffer
        return response

    def crawl(self):
        """
        Crawl the server for flags and links to visit using asynchronous requests
        """
        import concurrent.futures

        with concurrent.futures.ThreadPoolExecutor(max_workers=25) as executor:
            future_to_url = {executor.submit(self.process_url, url): url for url in self.need_to_visit}
            while future_to_url and len(self.secret_flags) < 5:
                # Wait for the next future to complete.
                done, _ = concurrent.futures.wait(
                    future_to_url, return_when=concurrent.futures.FIRST_COMPLETED)

                for future in done:
                    url = future_to_url[future]
                    try:
                        future.result()
                    except Exception as exc:
                        print('%r generated an exception: %s' % (url, exc))
                    # Remove this future from the dict so we don't attempt to process it again.
                    del future_to_url[future]

                # Add newly found URLs to the future_to_url dict
                for url in list(self.need_to_visit):
                    if url not in future_to_url.values():
                        future_to_url[executor.submit(self.process_url, url)] = url

                if len(self.secret_flags) >= 5:
                    break  # Stop if we've found all flags

    def process_url(self, current_url):
        """
        Process a single URL: fetch, parse, and extract data.
        """
        if current_url in self.url_visited:  # Skip if already visited
            return

        self.url_visited.add(current_url)  # Mark as visited
        response = self.send_request("GET", current_url)

        parser = FlagFinder()
        parser.feed(response)
        flags = parser.get_flags()
        links = parser.get_links()

        for flag in flags:
            if flag not in self.secret_flags:
                print(flag.replace("FLAG: ", ""))
                self.secret_flags.add(flag)

        for link in links:
            parsed_link = urlparse(link)
            if parsed_link.scheme == '' and parsed_link.netloc == '':
                link = urljoin(f"https://{self.server}", link)
            if self.server in link and link not in self.url_visited and link not in self.need_to_visit:
                self.need_to_visit.append(link)



class FlagFinder(HTMLParser):
    """
    FlagFinder class to find flags in HTML content

    Args:
        HTMLParser (_type_): _description_
    """
    def __init__(self):
        super().__init__()
        self.links = set()
        self.flags = set()  
        self.in_flag_tag = False  

    def handle_starttag(self, tag, attrs):
        """
        Handle the start tag of an HTML element and extract flags and links from it

        Args:
            tag (_type_): _description_
            attrs (_type_): _description_
        """
        if tag == "a": # If the tag is an anchor tag
            for attr, value in attrs: # Iterate over the attributes
                if attr == "href": # If the attribute is the href
                    self.links.add(value) # Add the link to the links

        if tag == "h3": # If the tag is an h3 tag
            attrs_dict = dict(attrs)  # Convert the attributes to a dictionary
            if attrs_dict.get("class") == "secret_flag": # If the class is secret_flag
                self.in_flag_tag = True  # Set the flag tag to true

    def handle_data(self, data):
        """
        Handle the data of an HTML element and extract flags from it

        Args:
            data (_type_): _description_
        """
        if self.in_flag_tag: # If the flag tag is true
            self.flags.add(data) # Add the data to the flags
            self.in_flag_tag = False # Set the flag tag to false

    def handle_endtag(self, tag):
        """
        Handle the end tag of an HTML element and reset the flag tag if necessary

        Args:
            tag (_type_): _description_
        """
        if tag == "h3" and self.in_flag_tag:
            self.in_flag_tag = False

    def get_links(self):
        return self.links

    def get_flags(self):
        return self.flags


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    crawler = Crawler(args)
    crawler.run()